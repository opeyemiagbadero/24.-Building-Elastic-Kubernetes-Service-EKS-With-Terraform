# Building Elastic Kubernetes Service (EKS) With Terraform #

In thiw project, we will use Terraform to create a Kubernetes EKS cluster and dynamically add scalable worker nodes,  deploy multiple applications using HELM, experience more kubernetes objects and how to use them with Helm. Such as Dynamic provisioning of volumes to make pods stateful.

# Building EKS with Terraform #

Open up a new directory on your laptop, and name it eks

Use AWS CLI to create an S3 bucket to store the terraform state: `aws s3 mb s3://opeyemi-eks-bucket-backend-terraform`

Create a file – backend.tf

```
terraform {
  backend "s3" {
    bucket = "opeyemi-eks-bucket-backend-terraform"
    key    = "terraform.tfstate"
    region = "us-east-1"
  }
}
```

![1 backend tf](https://github.com/opeyemiagbadero/24.-Building-Elastic-Kubernetes-Service-EKS-With-Terraform/assets/79456052/c71c6455-7098-4b0f-9ac1-1ee1ff6619fa)

Create a provider.tf file

![2 provider tf](https://github.com/opeyemiagbadero/24.-Building-Elastic-Kubernetes-Service-EKS-With-Terraform/assets/79456052/5e6ec507-ea90-405f-bbf3-2ddb8cb7ae6c)

Create a file – network.tf and provision Elastic IP for Nat Gateway.
```
# reserve Elastic IP to be used in our NAT gateway
resource "aws_eip" "nat_gw_elastic_ip" {
vpc = true

tags = {
Name            = "${var.cluster_name}-nat-eip"
iac_environment = var.iac_environment_tag
}
}
```

 Create VPC, Private and public subnets using the official AWS module.

```
module "vpc" {
source  = "terraform-aws-modules/vpc/aws"

name = "${var.name_prefix}-vpc"
cidr = var.main_network_block
azs  = data.aws_availability_zones.available_azs.names

private_subnets = [
# this loop will create a one-line list as ["10.0.0.0/20", "10.0.16.0/20", "10.0.32.0/20", ...]
# with a length depending on how many Zones are available
for zone_id in data.aws_availability_zones.available_azs.zone_ids :
cidrsubnet(var.main_network_block, var.subnet_prefix_extension, tonumber(substr(zone_id, length(zone_id) - 1, 1)) - 1)
]

public_subnets = [
# this loop will create a one-line list as ["10.0.128.0/20", "10.0.144.0/20", "10.0.160.0/20", ...]
# with a length depending on how many Zones are available
# there is a zone Offset variable, to make sure no collisions are present with private subnet blocks
for zone_id in data.aws_availability_zones.available_azs.zone_ids :
cidrsubnet(var.main_network_block, var.subnet_prefix_extension, tonumber(substr(zone_id, length(zone_id) - 1, 1)) + var.zone_offset - 1)
]

# Enable single NAT Gateway to save some money
# WARNING: this could create a single point of failure, since we are creating a NAT Gateway in one AZ only
# feel free to change these options if you need to ensure full Availability without the need of running 'terraform apply'
# reference: https://registry.terraform.io/modules/terraform-aws-modules/vpc/aws/2.44.0#nat-gateway-scenarios
enable_nat_gateway     = true
single_nat_gateway     = true
one_nat_gateway_per_az = false
enable_dns_hostnames   = true
reuse_nat_ips          = true
external_nat_ip_ids    = [aws_eip.nat_gw_elastic_ip.id]

# Add VPC/Subnet tags required by EKS
tags = {
"kubernetes.io/cluster/${var.cluster_name}" = "shared"
iac_environment                             = var.iac_environment_tag
}
public_subnet_tags = {
"kubernetes.io/cluster/${var.cluster_name}" = "shared"
"kubernetes.io/role/elb"                    = "1"
iac_environment                             = var.iac_environment_tag
}
private_subnet_tags = {
"kubernetes.io/cluster/${var.cluster_name}" = "shared"
"kubernetes.io/role/internal-elb"           = "1"
iac_environment                             = var.iac_environment_tag
}
}
```
![3 network tf](https://github.com/opeyemiagbadero/24.-Building-Elastic-Kubernetes-Service-EKS-With-Terraform/assets/79456052/01bb3b05-707d-4867-b75e-ae206983c530)

Note: The tags added to the subnets is very important. The Kubernetes Cloud Controller Manager (cloud-controller-manager) and AWS Load Balancer Controller (aws-load-balancer-controller) needs to identify the cluster’s. To do that, it querries the cluster’s subnets by using the tags as a filter.

For public and private subnets that use load balancer resources: each subnet must be tagged

```
Key: kubernetes.io/cluster/cluster-name
Value: shared
```

For private subnets that use internal load balancer resources: each subnet must be tagged

```
Key: kubernetes.io/role/internal-elb
Value: 1
```

For public subnets that use internal load balancer resources: each subnet must be tagged

```
Key: kubernetes.io/role/elb
Value: 1
```

Create a file – variables.tf

```
# create some variables
variable "cluster_name" {
type        = string
description = "EKS cluster name."
}
variable "iac_environment_tag" {
type        = string
description = "AWS tag to indicate environment name of each infrastructure object."
}
variable "name_prefix" {
type        = string
description = "Prefix to be used on each infrastructure object Name created in AWS."
}
variable "main_network_block" {
type        = string
description = "Base CIDR block to be used in our VPC."
}
variable "subnet_prefix_extension" {
type        = number
description = "CIDR block bits extension to calculate CIDR blocks of each subnetwork."
}
variable "zone_offset" {
type        = number
description = "CIDR block bits extension offset to calculate Public subnets, avoiding collisions with Private subnets."
}
```

![4 variable tf](https://github.com/opeyemiagbadero/24.-Building-Elastic-Kubernetes-Service-EKS-With-Terraform/assets/79456052/677e46d2-7c1c-4985-9fdc-e14840ebf07e)

Create a file – data.tf – This will pull the available AZs for use.

```
# get all available AZs in our region
data "aws_availability_zones" "available_azs" {
state = "available"
}
data "aws_caller_identity" "current" {} # used for accesing Account ID and ARN
```

![5 data tf](https://github.com/opeyemiagbadero/24.-Building-Elastic-Kubernetes-Service-EKS-With-Terraform/assets/79456052/cffad859-6be6-40b9-8473-d8ef5d4bff5b)

Create a file – eks.tf and provision EKS cluster

```
module "eks_cluster" {
  source  = "terraform-aws-modules/eks/aws"
  version = "~> 18.0"
  cluster_name    = var.cluster_name
  cluster_version = "1.22"
  vpc_id     = module.vpc.vpc_id
  subnet_ids = module.vpc.private_subnets
  cluster_endpoint_private_access = true
  cluster_endpoint_public_access = true

  # Self Managed Node Group(s)
  self_managed_node_group_defaults = {
    instance_type                          = var.asg_instance_types[0]
    update_launch_template_default_version = true
  }
  self_managed_node_groups = local.self_managed_node_groups

  # aws-auth configmap
  create_aws_auth_configmap = true
  manage_aws_auth_configmap = true
  aws_auth_users = concat(local.admin_user_map_users, local.developer_user_map_users)
  tags = {
    Environment = "prod"
    Terraform   = "true"
  }
}
```

![6 eks tf](https://github.com/opeyemiagbadero/24.-Building-Elastic-Kubernetes-Service-EKS-With-Terraform/assets/79456052/6b5c5fc5-72f8-427e-a129-7dad6fe26f16)

Create a file – locals.tf to create local variables. Terraform does not allow assigning variable to variables. There is good reasons for that to avoid repeating your code unecessarily. So a terraform way to achieve this would be to use locals so that your code can be kept DRY

```
# render Admin & Developer users list with the structure required by EKS module
locals {
  admin_user_map_users = [
    for admin_user in var.admin_users :
    {
      userarn  = "arn:aws:iam::${data.aws_caller_identity.current.account_id}:user/${admin_user}"
      username = admin_user
      groups   = ["system:masters"]
    }
  ]
  developer_user_map_users = [
    for developer_user in var.developer_users :
    {
      userarn  = "arn:aws:iam::${data.aws_caller_identity.current.account_id}:user/${developer_user}"
      username = developer_user
      groups   = ["${var.name_prefix}-developers"]
    }
  ]

  self_managed_node_groups = {
    worker_group1 = {
      name = "${var.cluster_name}-wg"

      min_size      = var.autoscaling_minimum_size_by_az * length(data.aws_availability_zones.available_azs.zone_ids)
      desired_size      = var.autoscaling_minimum_size_by_az * length(data.aws_availability_zones.available_azs.zone_ids)
      max_size  = var.autoscaling_maximum_size_by_az * length(data.aws_availability_zones.available_azs.zone_ids)
      instance_type = var.asg_instance_types[0].instance_type

      bootstrap_extra_args = "--kubelet-extra-args '--node-labels=node.kubernetes.io/lifecycle=spot'"

      block_device_mappings = {
        xvda = {
          device_name = "/dev/xvda"
          ebs = {
            delete_on_termination = true
            encrypted             = false
            volume_size           = 10
            volume_type           = "gp2"
          }
        }
      }

      use_mixed_instances_policy = true
      mixed_instances_policy = {
        instances_distribution = {
          spot_instance_pools = 4
        }

        override = var.asg_instance_types
      }
    }
  }
}

```

![7 locals tf](https://github.com/opeyemiagbadero/24.-Building-Elastic-Kubernetes-Service-EKS-With-Terraform/assets/79456052/fd1b8fb0-9a62-4249-aeb9-bc817d8bd6b6)

Add more variables to the variables.tf file

```
# create some variables
variable "admin_users" {
  type        = list(string)
  description = "List of Kubernetes admins."
}
variable "developer_users" {
  type        = list(string)
  description = "List of Kubernetes developers."
}
variable "asg_instance_types" {
  description = "List of EC2 instance machine types to be used in EKS."
}
variable "autoscaling_minimum_size_by_az" {
  type        = number
  description = "Minimum number of EC2 instances to autoscale our EKS cluster on each AZ."
}
variable "autoscaling_maximum_size_by_az" {
  type        = number
  description = "Maximum number of EC2 instances to autoscale our EKS cluster on each AZ."
}
```

![8  Add more variables to the variables tf file](https://github.com/opeyemiagbadero/24.-Building-Elastic-Kubernetes-Service-EKS-With-Terraform/assets/79456052/0086b4aa-4422-4f84-a5ab-d11e54c32339)

Create a file – variables.tfvars to set values for variables.

```
cluster_name            = "tooling-app-eks"
iac_environment_tag     = "development"
name_prefix             = "darey-io-eks"
main_network_block      = "10.0.0.0/16"
subnet_prefix_extension = 4
zone_offset             = 8

# Ensure that these users already exist in AWS IAM. Another approach is that you can introduce an iam.tf file to manage users separately, get the data source and interpolate their ARN.
admin_users                              = ["dare", "solomon"]
developer_users                          = ["leke", "david"]
asg_instance_types                       = ["t3.small", "t2.small"]
autoscaling_minimum_size_by_az           = 1
autoscaling_maximum_size_by_az           = 10
autoscaling_average_cpu                  = 30
```
![9  terraform  tfvars also known as variables tfvars](https://github.com/opeyemiagbadero/24.-Building-Elastic-Kubernetes-Service-EKS-With-Terraform/assets/79456052/210a18c1-c934-4d9e-8b35-787afaa43aca)

Run terraform init and  Run Terraform plan – Your plan should have an output

![10 terraform init](https://github.com/opeyemiagbadero/24.-Building-Elastic-Kubernetes-Service-EKS-With-Terraform/assets/79456052/9abedab0-2dff-4887-8c43-f0fc29bcd3ff)

![11 terraform plan](https://github.com/opeyemiagbadero/24.-Building-Elastic-Kubernetes-Service-EKS-With-Terraform/assets/79456052/ab72f622-5876-4236-a56a-166868fbadb8)

Run Terraform apply

This will begin to create cloud resources, and fail at some point with the error. That is because for us to connect to the cluster using the kubeconfig, Terraform needs to be able to connect and set the credentials correctly.

```
╷
│ Error: Post "http://localhost/api/v1/namespaces/kube-system/configmaps": dial tcp [::1]:80: connect: connection refused
│ 
│   with module.eks-cluster.kubernetes_config_map.aws_auth[0],
│   on .terraform/modules/eks-cluster/aws_auth.tf line 63, in resource "kubernetes_config_map" "aws_auth":
│   63: resource "kubernetes_config_map" "aws_auth" {

```

![12a Run Terraform apply
This will begin to create cloud resources, and fail at some point with the error](https://github.com/opeyemiagbadero/24.-Building-Elastic-Kubernetes-Service-EKS-With-Terraform/assets/79456052/865347e0-aa70-4dc5-b209-c3176157401c)


To fix the problem append the data.tf and the provider.tf files

Append the file data.tf

```
# get EKS cluster info to configure Kubernetes and Helm providers
data "aws_eks_cluster" "cluster" {
  name = module.eks_cluster.cluster_id
}
data "aws_eks_cluster_auth" "cluster" {
  name = module.eks_cluster.cluster_id
}
```
![13  append the  data tf file](https://github.com/opeyemiagbadero/24.-Building-Elastic-Kubernetes-Service-EKS-With-Terraform/assets/79456052/96c326b0-ef4a-487a-b409-165c5bed2a37)

Append the file provider.tf

```
# get EKS authentication for being able to manage k8s objects from terraform
provider "kubernetes" {
  host                   = data.aws_eks_cluster.cluster.endpoint
  cluster_ca_certificate = base64decode(data.aws_eks_cluster.cluster.certificate_authority.0.data)
  token                  = data.aws_eks_cluster_auth.cluster.token
}
```

![14  append the provider tf file](https://github.com/opeyemiagbadero/24.-Building-Elastic-Kubernetes-Service-EKS-With-Terraform/assets/79456052/58f05c46-bec8-452e-a603-dde72c57ece2)

Run the init and plan again – This time you will see 

```
 # module.eks-cluster.kubernetes_config_map.aws_auth[0] will be created
  + resource "kubernetes_config_map" "aws_auth" {
      + data = {
          + "mapAccounts" = jsonencode([])
          + "mapRoles"    = <<-EOT
                - "groups":
                  - "system:bootstrappers"
                  - "system:nodes"
                  "rolearn": "arn:aws:iam::696742900004:role/tooling-app-eks20210718113602300300000009"
                  "username": "system:node:{{EC2PrivateDNSName}}"
            EOT
          + "mapUsers"    = <<-EOT
                - "groups":
                  - "system:masters"
                  "userarn": "arn:aws:iam::696742900004:user/dare"
                  "username": "dare"
                - "groups":
                  - "system:masters"
                  "userarn": "arn:aws:iam::696742900004:user/solomon"
                  "username": "solomon"
                - "groups":
                  - "darey-io-eks-developers"
                  "userarn": "arn:aws:iam::696742900004:user/leke"
                  "username": "leke"
                - "groups":
                  - "darey-io-eks-developers"
                  "userarn": "arn:aws:iam::696742900004:user/david"
                  "username": "david"
            EOT
        }
      + id   = (known after apply)

      + metadata {
          + generation       = (known after apply)
          + labels           = {
              + "app.kubernetes.io/managed-by" = "Terraform"
              + "terraform.io/module"          = "terraform-aws-modules.eks.aws"
            }
          + name             = "aws-auth"
          + namespace        = "kube-system"
          + resource_version = (known after apply)
          + uid              = (known after apply)
        }
    }
```

![15  run init again](https://github.com/opeyemiagbadero/24.-Building-Elastic-Kubernetes-Service-EKS-With-Terraform/assets/79456052/0bf98ef5-3b61-4ba5-9d65-d3f43d9b14cd)

![16 display of the kubernetes resource under terraform plan](https://github.com/opeyemiagbadero/24.-Building-Elastic-Kubernetes-Service-EKS-With-Terraform/assets/79456052/e87fd39c-6220-420c-b326-c1455e2dc4a5)

Create kubeconfig file using awscli.

`aws eks update-kubecofig --name tooling-app-eks --region us-east-1 --kubeconfig kubeconfig`

![17 Create kubeconfig file using awscli 
](https://github.com/opeyemiagbadero/24.-Building-Elastic-Kubernetes-Service-EKS-With-Terraform/assets/79456052/af3ad014-3621-4ba2-8898-74a94a2f5698)


# Installing Helm #

Download the tar.gz file from the project’s Github release page. Or simply use wget to download version 3.6.3 directly `wget https://github.com/helm/helm/archive/refs/tags/v3.6.3.tar.gz`

Unpack the tar.gz file `tar -zxvf v3.6.3.tar.gz `

cd into the unpacked directory  `cd helm-3.6.3`

Build the source code using make utility `make build`

Helm binary will be in the bin folder. Simply move it to the bin directory on your system. `sudo mv bin/helm /usr/local/bin/`

Check that Helm is installed

![18  Confirm helm version](https://github.com/opeyemiagbadero/24.-Building-Elastic-Kubernetes-Service-EKS-With-Terraform/assets/79456052/46488169-1b10-492c-b998-7f908a1f5ac6)

# Deploy Jenkins with Helm #

One of the amazing things about helm is the fact that you can deploy applications that are already packaged from a public helm repository directly with very minimal configuration. An example is Jenkins.

- Visit Artifact Hub to find packaged applications as Helm Charts
- Search for Jenkins
- Add the repository to helm so that you can easily download and deploy `helm repo add jenkins https://charts.jenkins.io`
- Update helm repo `helm repo update` 
- Install the chart `helm install myjenkins  jenkins/jenkins --kubeconfig kubeconfig file`

![21  Add the repository to helm so that you can easily download and deploy
  Update helm repo
 Install the chart
  Output](https://github.com/opeyemiagbadero/24.-Building-Elastic-Kubernetes-Service-EKS-With-Terraform/assets/79456052/0da55a2a-8f88-43a6-bfa5-60ba6c067bff)


Check the Helm deployment `helm ls --kubeconfig [kubeconfig file]`

output 
```
NAME    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION
jenkins default         1               2021-08-01 12:38:53.429471 +0100 BST    deployed        jenkins-3.5.9   2.289.3
```

Check the pods `kubectl get pods --kubeconfigo [kubeconfig file]`

Output: 
```
NAME        READY   STATUS    RESTARTS   AGE
jenkins-0   2/2     Running   0          6m14s
```

Describe and check the logs of the running pod using the commands `kubectl describe pod jenkins-0 --kubeconfig [kubeconfig file]` and `kubectl logs jenkins-0 --kubeconfig [kubeconfig file]`

You will notice an output with an error
```
error: a container name must be specified for pod jenkins-0, choose one of: [jenkins config-reload] or one of the init containers: [init]
```

This is because the pod has a Sidecar container alongside with the Jenkins container. As you can see fromt he error output, there is a list of containers inside the pod [jenkins config-reload] i.e jenkins and config-reload containers. The job of the config-reload is mainly to help Jenkins to reload its configuration without recreating the pod.

Therefore we need to let kubectl know, which pod we are interested to see its log. Hence, the command will be updated like: `kubectl logs jenkins-0 -c jenkins --kubeconfig [kubeconfig file]`

Now lets avoid calling the [kubeconfig file] everytime. Kubectl expects to find the default kubeconfig file in the location ~/.kube/config. But what if you already have another cluster using that same file? It doesn’t make sense to overwrite it. What you will do is to merge all the kubeconfig files together using a kubectl plugin called [konfig](https://github.com/corneliusweig/konfig) and select whichever one you need to be active.

Install a package manager for kubectl called krew so that it will enable you to install plugins to extend the functionality of kubectl. 

Install the [konfig plugin](https://github.com/corneliusweig/konfig) `  kubectl krew install konfig`

Import the kubeconfig into the default kubeconfig file. Ensure to accept the prompt to overide. `  sudo kubectl konfig import --save  [kubeconfig file]`

Show all the contexts – Meaning all the clusters configured in your kubeconfig. If you have more than 1 Kubernetes clusters configured, you will see them all in the output. `  kubectl config get-contexts`

Set the current context to use for all kubectl and helm commands `  kubectl config use-context [name of EKS cluster]`

Test that it is working without specifying the --kubeconfig flag `  kubectl get po`

output:
```
  NAME        READY   STATUS    RESTARTS   AGE
  jenkins-0   2/2     Running   0          84m
```
Display the current context. This will let you know the context in which you are using to interact with Kubernetes. `  kubectl config current-context`

Now that we can use kubectl without the --kubeconfig flag, Lets get access to the Jenkins UI. 

There are some commands that was provided on the screen when Jenkins was installed with Helm. See number 5 above. Get the password to the admin user 
```
kubectl exec --namespace default -it svc/jenkins -c jenkins -- /bin/cat /run/secrets/chart-admin-password && echo
```
Use port forwarding to access Jenkins from the UI `  kubectl --namespace default port-forward svc/jenkins 8080:8080`

![21d  legit snapshot to use ](https://github.com/opeyemiagbadero/24.-Building-Elastic-Kubernetes-Service-EKS-With-Terraform/assets/79456052/f393ad1a-2cb6-4681-8f20-20c23ef04b2a)

Go to the browser localhost:8080 and authenticate with the username and password.

![22  Jenkins login page](https://github.com/opeyemiagbadero/24.-Building-Elastic-Kubernetes-Service-EKS-With-Terraform/assets/79456052/339502b5-ecc5-4f2a-874a-71368260f930)


![23  Welcome to Jenkins](https://github.com/opeyemiagbadero/24.-Building-Elastic-Kubernetes-Service-EKS-With-Terraform/assets/79456052/5f197431-8f42-4ac4-8e41-af7334837744)


# Quick task #

To install Prometheus, Nginx, and Grafana, you can visit the ArtifactHub website and follow the instructions provided to download the respective Helm charts.

Access the ArtifactHub website.
- Search for the desired charts, such as "Nginx," and "Grafana."
- Locate the Helm charts for each application and click on their respective links.
- On the chart page, you will find installation instructions specific to each chart.
- Follow the provided instructions to download and install the Helm charts using the Helm package manager.

![26  nginx backend config](https://github.com/opeyemiagbadero/24.-Building-Elastic-Kubernetes-Service-EKS-With-Terraform/assets/79456052/f9421032-c3ea-4b1a-ad16-5aff88fea93e)

![24 Nginx using helm chart](https://github.com/opeyemiagbadero/24.-Building-Elastic-Kubernetes-Service-EKS-With-Terraform/assets/79456052/ed865971-a91c-4cf5-b05a-e37d82f53a5d)


![27  backend for grafana](https://github.com/opeyemiagbadero/24.-Building-Elastic-Kubernetes-Service-EKS-With-Terraform/assets/79456052/e1f4dc9d-6388-40a3-8197-ce1798b5284c)


![28  Grafana port forwarding](https://github.com/opeyemiagbadero/24.-Building-Elastic-Kubernetes-Service-EKS-With-Terraform/assets/79456052/97fe7943-6ee6-448e-a40e-a2ea5a4e70b5)

![29  logged into grafana](https://github.com/opeyemiagbadero/24.-Building-Elastic-Kubernetes-Service-EKS-With-Terraform/assets/79456052/f2c6b410-441f-4087-9e2d-dfba5ac75408)































